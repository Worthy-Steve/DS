# Statement 16
# Multiclass classification using Deep Neural Networks: Example: Use the OCR letter recognition dataset/Alphabet.csv

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
import time

# ðŸ”¹ Step 1: Load the dataset
data = pd.read_csv("alphabet_ocr.csv")  # Replace with actual file name

# ðŸ”¹ Step 2: Extract features and labels
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

# ðŸ”¹ Step 3: Encode labels (Aâ€“Z â†’ 0â€“25)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
y_categorical = to_categorical(y_encoded, num_classes=26)

# ðŸ”¹ Step 4: Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ðŸ”¹ Step 5: Split into training and testing
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_categorical, test_size=0.2, random_state=42)

# ðŸ”¹ Step 6: Define the DNN model
model = Sequential()
model.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(26, activation='softmax'))  # 26 output classes

# ðŸ”¹ Step 7: Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# ðŸ”¹ Step 8: Train the model
start = time.time()
history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=1)
end = time.time()

# ðŸ”¹ Step 9: Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nâœ… Training Time: {end - start:.2f} seconds")
print(f"âœ… Test Accuracy: {accuracy * 100:.2f}%")
print(f"âœ… Test Loss: {loss:.4f}")